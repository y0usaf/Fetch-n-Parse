<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Web Scraper Demo</title>
  <script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/cheerio/1.0.0/cheerio.min.js"></script>
</head>
<body>
  <h1>Web Scraper Demo</h1>
  <button id="scrape-btn">Start Scraping</button>
  <pre id="output"></pre>

  <script>
    /**
     * Fetches the content of a given URL using the axios library.
     *
     * @param {string} url - The URL to fetch the content from.
     * @returns {Promise<string>} - A promise resolving to the fetched content.
     */
    async function fetchContent(url) {
      try {
        const response = await axios.get(url);
        return response.data;
      } catch (error) {
        console.error(`Error fetching content from ${url}:`, error);
        return '';
      }
    }

    /**
     * Parses the content using Cheerio and extracts the desired data based on the provided rules.
     *
     * @param {string} content - The HTML content to parse.
     * @param {Object} rules - An object defining the parsing rules.
     * @returns {Object} - An object containing the extracted data.
     */
    function parseContent(content, rules) {
      const $ = cheerio.load(content);
      const extractedData = {};

      for (const [key, rule] of Object.entries(rules)) {
        if (rule.type === 'text') {
          extractedData[key] = $(rule.selector).text().trim();
        } else if (rule.type === 'attr') {
          extractedData[key] = $(rule.selector).attr(rule.attribute);
        } else if (rule.type === 'list') {
          const items = [];
          $(rule.selector).each((_, element) => {
            const itemData = parseContent($(element).html(), rule.itemRules);
            items.push(itemData);
          });
          extractedData[key] = items;
        }
      }

      return extractedData;
    }

    /**
     * Main function that sets the target URL, parsing rules, and runs the web scraper.
     */
    async function main() {
      // Configure the target URL and parsing rules.
      const config = {
        url: 'https://example.com',
        rules: {
          title: {
            type: 'text',
            selector: 'h1'
          },
          imageUrl: {
            type: 'attr',
            selector: 'img.main-image',
            attribute: 'src'
          },
          items: {
            type: 'list',
            selector: '.item-list li',
            itemRules: {
              name: {
                type: 'text',
                selector: '.item-name'
              },
              price: {
                type: 'text',
                selector: '.item-price'
              }
            }
          }
        }
      };

      // Fetch and parse the content.
      const content = await fetchContent(config.url);
      const data = parseContent(content, config.rules);

      // Output the extracted data.
      document.getElementById('output').textContent = JSON.stringify(data, null, 2);
    }

    document.getElementById('scrape-btn').addEventListener('click', main);
  </script>
</body>
</html>
